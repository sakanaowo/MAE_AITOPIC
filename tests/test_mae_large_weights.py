"""
Test MAE ViT-Large â€” Kiá»ƒm thá»­ load trá»ng sá»‘ pretrained
========================================================
1. Táº¡o model MAEViTLarge
2. So sÃ¡nh state_dict keys vá»›i checkpoint
3. Load trá»ng sá»‘ vÃ  cháº¡y forward pass
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import torch
from mae.large import MAEViTLarge, mae_vit_large_patch16


def test_key_matching():
    """So sÃ¡nh state_dict keys giá»¯a model vÃ  checkpoint."""
    print("=" * 60)
    print("TEST 1: So sÃ¡nh state_dict keys")
    print("=" * 60)
    
    # Táº¡o model
    model = mae_vit_large_patch16()
    model_keys = set(model.state_dict().keys())
    
    # Load checkpoint
    ckpt_path = os.path.join(os.path.dirname(__file__), '..', 'data', 'mae_pretrain_vit_large.pth')
    checkpoint = torch.load(ckpt_path, map_location='cpu')
    ckpt_keys = set(checkpoint['model'].keys())
    
    # Keys trong checkpoint nhÆ°ng KHÃ”NG trong model
    extra_in_ckpt = ckpt_keys - model_keys
    if extra_in_ckpt:
        print(f"âŒ {len(extra_in_ckpt)} keys trong checkpoint KHÃ”NG cÃ³ trong model:")
        for k in sorted(extra_in_ckpt):
            print(f"   - {k}")
    else:
        print(f"âœ… Táº¥t cáº£ {len(ckpt_keys)} checkpoint keys Ä‘á»u cÃ³ trong model")
    
    # Keys trong model nhÆ°ng KHÃ”NG trong checkpoint (pháº£i toÃ n decoder)
    extra_in_model = model_keys - ckpt_keys
    decoder_keys = [k for k in extra_in_model if any(
        k.startswith(p) for p in ['decoder_', 'mask_token']
    )]
    encoder_missing = [k for k in extra_in_model if k not in decoder_keys]
    
    print(f"ðŸ“‹ {len(decoder_keys)} decoder keys chá»‰ cÃ³ trong model (bÃ¬nh thÆ°á»ng)")
    if encoder_missing:
        print(f"âŒ {len(encoder_missing)} encoder keys chá»‰ cÃ³ trong model (Lá»–I!):")
        for k in sorted(encoder_missing):
            print(f"   - {k}")
    
    # Kiá»ƒm tra shape
    ckpt_sd = checkpoint['model']
    model_sd = model.state_dict()
    shape_mismatch = []
    for k in ckpt_keys & model_keys:
        if ckpt_sd[k].shape != model_sd[k].shape:
            shape_mismatch.append((k, ckpt_sd[k].shape, model_sd[k].shape))
    
    if shape_mismatch:
        print(f"âŒ {len(shape_mismatch)} shape mismatches:")
        for k, cs, ms in shape_mismatch:
            print(f"   - {k}: ckpt={cs} vs model={ms}")
    else:
        print(f"âœ… Táº¥t cáº£ shapes khá»›p")
    
    assert len(extra_in_ckpt) == 0, "CÃ³ keys trong checkpoint khÃ´ng cÃ³ trong model!"
    assert len(encoder_missing) == 0, "CÃ³ encoder keys thiáº¿u!"
    assert len(shape_mismatch) == 0, "CÃ³ shape mismatch!"
    print("âœ… TEST 1 PASSED\n")


def test_load_weights():
    """Load trá»ng sá»‘ pretrained vÃ o model."""
    print("=" * 60)
    print("TEST 2: Load trá»ng sá»‘ pretrained")
    print("=" * 60)
    
    model = mae_vit_large_patch16()
    ckpt_path = os.path.join(os.path.dirname(__file__), '..', 'data', 'mae_pretrain_vit_large.pth')
    
    missing, unexpected = model.load_pretrained_encoder(ckpt_path)
    
    # Kiá»ƒm tra khÃ´ng cÃ³ unexpected keys
    assert len(unexpected) == 0, f"Unexpected keys: {unexpected}"
    
    # Kiá»ƒm tra missing keys toÃ n decoder
    for k in missing:
        assert any(k.startswith(p) for p in ['decoder_', 'mask_token']), \
            f"Encoder key missing: {k}"
    
    print("âœ… TEST 2 PASSED\n")


def test_forward_pass():
    """Cháº¡y forward pass vá»›i trá»ng sá»‘ Ä‘Ã£ load."""
    print("=" * 60)
    print("TEST 3: Forward pass vá»›i trá»ng sá»‘ pretrained")
    print("=" * 60)
    
    model = mae_vit_large_patch16()
    ckpt_path = os.path.join(os.path.dirname(__file__), '..', 'data', 'mae_pretrain_vit_large.pth')
    model.load_pretrained_encoder(ckpt_path)
    model.eval()
    
    # Forward pass
    x = torch.randn(2, 3, 224, 224)
    with torch.no_grad():
        loss, pred, mask = model(x, mask_ratio=0.75)
    
    print(f"Input shape:  {x.shape}")
    print(f"Pred shape:   {pred.shape}")
    print(f"Mask shape:   {mask.shape}")
    print(f"Loss:         {loss.item():.4f}")
    print(f"Mask ratio:   {mask.sum() / mask.numel():.2%}")
    
    # Kiá»ƒm tra shapes
    assert pred.shape == (2, 196, 768), f"Pred shape sai: {pred.shape}"
    assert mask.shape == (2, 196), f"Mask shape sai: {mask.shape}"
    assert not torch.isnan(loss), "Loss lÃ  NaN!"
    assert not torch.isinf(loss), "Loss lÃ  Inf!"
    
    # Encoder only test
    with torch.no_grad():
        latent, mask2, ids_restore = model.forward_encoder(x, mask_ratio=0.75)
    print(f"Encoder output: {latent.shape}")
    assert latent.shape[2] == 1024, f"Encoder dim sai: {latent.shape[2]}"
    
    print("âœ… TEST 3 PASSED\n")


def test_param_count():
    """Kiá»ƒm tra sá»‘ parameters."""
    print("=" * 60)
    print("TEST 4: Parameter count")
    print("=" * 60)
    
    model = mae_vit_large_patch16()
    
    total_params = sum(p.numel() for p in model.parameters())
    encoder_params = sum(
        p.numel() for n, p in model.named_parameters()
        if not any(n.startswith(prefix) for prefix in ['decoder_', 'mask_token'])
    )
    decoder_params = total_params - encoder_params
    
    print(f"Total params:   {total_params / 1e6:.1f}M")
    print(f"Encoder params: {encoder_params / 1e6:.1f}M")
    print(f"Decoder params: {decoder_params / 1e6:.1f}M")
    
    # ViT-Large encoder ~ 304M params
    assert encoder_params > 300e6, f"Encoder params quÃ¡ Ã­t: {encoder_params / 1e6:.1f}M"
    
    print("âœ… TEST 4 PASSED\n")


if __name__ == "__main__":
    print("ðŸš€ Báº¯t Ä‘áº§u kiá»ƒm thá»­ MAE ViT-Large\n")
    
    test_key_matching()
    test_load_weights()
    test_forward_pass()
    test_param_count()
    
    print("=" * 60)
    print("ðŸŽ‰ Táº¤T Cáº¢ TESTS PASSED!")
    print("=" * 60)
